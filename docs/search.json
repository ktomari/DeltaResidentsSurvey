[
  {
    "objectID": "doc_missing_and_ordinal.html",
    "href": "doc_missing_and_ordinal.html",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "",
    "text": "In this chapter, we review the types of missing data and ordered (or ordinal) variables present in the Delta Residents Survey (DRS) data set. As this documentation is intended to serve all different versions of the DRS data set, you may find some descriptions that do not apply to the version of the data set to which you have access. Irrespective, this chapter should satisfy questions about whichever data set you have."
  },
  {
    "objectID": "doc_missing_and_ordinal.html#history-of-drs-data",
    "href": "doc_missing_and_ordinal.html#history-of-drs-data",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "1 History of DRS Data",
    "text": "1 History of DRS Data\nThe DRS involved a large team of collaborators, advisers, and contractors. The implementation of the survey was done through the Qualtrics web service, while the de-identification and weighting of the data was performed by the team at the Institute for Social Research (ISR) at the California State University, Sacramento, with the involvement of their sub-contractor Marketing Systems Group (MSG). As such, the original data collection, and a portion of the data preparation was completed before the authors of this chapter had access to the data set. Although it is not strictly accurate to describe the product of ISR’s work as “raw data,” this is how we’ll address it for the remainder of this chapter.\nThe raw data was provided to us in the SPSS .sav format. As non-SPSS users, the core DRS team has little familiarity with the intricacies of this data format. We found that the sav format consists of a unique approach for conveying metadata that is moderately challenging to access in the programming language we use, R. The R language does not have a innate capacity to handle sav formats, and as such we used the {haven} package. While both R {haven} and the SPSS .sav format are sufficiently documented, we considered that the sav data format might pose a barrier to researchers interested in the DRS data set as it requires a basic understanding of object-oriented programming systems. In order to reduce this burden, we chose to supply the data in a simpler form (as a CSV), and provide an R script that does the heavy lifting with regard to the advanced features of sav data.\nThe primary concern with the sav format and its manipulation in R is the manner in which certain metadata are embedded directly into each column of the table of data. For instance, an ordinal, diverging- or likert-scale variable, with values that range from \"Very Satisfied\" to \"Very Disatisfied\", has an inherent structure that affects the way the data are processed. Since the order matters, each “label” (eg. \"Satisfied\") is coupled with a numeric encoding (eg. 2). Additionally, Qualtrics provides nuanced information regarding how survey respondents choose not to respond. This matter is discussed at length in the next section. In summary, the raw data format, sav, includes a complex data structure that incorporates both order and alternative approaches to conveying missing data that poses a challenge to casual R users."
  },
  {
    "objectID": "doc_missing_and_ordinal.html#final-data-format",
    "href": "doc_missing_and_ordinal.html#final-data-format",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "2 Final Data Format",
    "text": "2 Final Data Format\nThe final data product that the DRS team provides primarily consists of a Comma-Separated Values file, or CSV. These files are known as “flat files”, meaning they are simple text files that rely on a combination of commas and new lines to convey a tabular data structure. In simpler terms, CSV files can be opened in any basic text editor because it is written as plain text. The pitfall with CSV files is that they do not convey any information on the attributes of columns present in the table. While R’s read.csv() or readr::read_csv() have options for classifying a column by the base data types in the language, eg. the R class “character” or “factor”, the CSV format itself does not incorporate this information directly. This leaves some algorithm (ie. function) to do the guesswork of determining which R class is appropriate, or the user to define it manually. These issues are further discussed in the following two sections.\n\n2.1 Ordinal Variables\nIn the case where one reads a CSV file and applies the class “factor” to a column, it correctly converts the column into a categorical variable. However, such a process haphazardly applies an order that is based purely on the sequence by which the algorithm first encounters each category. For instance, the command factor(c(\"Satisfactory\", \"Unsure\", \"Very Satisfactory\")) yields a factor (or categorical) variable in R, but the “levels” are assigned in the order they were inscribed. In this case, the first category is \"Satisfactory\", even though it should be \"Very Satisfactory\". This classification also fails to specify levels (or categories), like \"Unsatisfactory\", that are absent in the initial factor() command execution. While we can manually adjust all of these issues, the important point is that these matters are not addressed directly by the CSV format.\n\n\n2.2 Missing Data\nQualtrics has the ability to convey missing data according to the way its absence is recorded by the web service. These include matters like, “Did the respondent see the question, but then decline to answer it?” Or, “Did the respondent not see the question?” These different types of missing data (or as described in other DRS documents as “missingness”) are embedded into the structure of the raw sav data in a way which cannot be reproduced by a CSV file. In the following section we discuss both the way in which the DRS team accommodated issues around order and missingness, and we provide a list of all the different types of missing data."
  },
  {
    "objectID": "doc_missing_and_ordinal.html#the-schema-of-the-drs-data-product",
    "href": "doc_missing_and_ordinal.html#the-schema-of-the-drs-data-product",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "3 The Schema of the DRS Data Product",
    "text": "3 The Schema of the DRS Data Product\nThe final data output of the DRS project is a combination of three files:\n\nA metadata file written as a Microsoft xlsx file.\nA CSV data file.\nA hash value stored in a txt file.\n\nThe purpose of the hash file is not discussed at length in this chapter, but a brief description is provided as a footnote 1. As discussed previously, the CSV file consists of the bulk of the recorded survey data, however the structure of this data is stored in the xlsx file. The xlsx, or metadata, contains information on each column/variable in the CSV file. Each variable in the metadata includes, wherever appropriate, the order and factors of the variable. As a means to reproduce the different types of missing data as recorded by Qualtrics, we provide different categories/factors for variables that have this information. The table below describes each possible missing value.\n\n\n\n\n\n\n\nR Value\nMeaning\n\n\n\n\nNA\nKnown as “system missing” these values indicate survey respondents never read the question, or stated differently, were never shown the question. Typically, this means the respondents ended the survey without completing it.\n\n\n&lt;Decline to answer&gt;\nOriginally, this value was recorded as 99 in the raw data. It means, respondents read the question, but declined to select a response, or enter anything in text entry field.\n\n\n&lt;Not Applicable&gt;\nOriginally, this value was recorded as 98 in the raw data. It typically means the respondent decided to end the survey on Question 39, which asks if the respondent would like to answer supplemental questions. However, in certain circumstances (ie. in questions preceding Q39), this can mean the respondent entered “NA”.\n\n\n&lt;I don't know&gt;\nOriginally, this value was recorded as 97, with varying textual descriptions, such as \"I'm not sure\" in the raw data. These values were selected from a list of options for a question the survey. In other words, it was a choice in a multiple-choice question. Unlike the previous missing values, this one was intentionally selected by the respondent.\n\n\n&lt;Missing&gt;\nThis value was recorded as \"Missing\" in a variable that was created by ISR or MSG. These variables are demographic variables that simplified responses to a question in the survey to either reduce identifiability or transcribe the survey responses in a way that matches census or marketing data for the purpose of constructing survey weights. In either case, &lt;Missing&gt; merely represents a value that was either dropped or was never provided by a respondent.\n\n\n&lt;Erased&gt;\nThis value was created by the current authors, ie. the part of the team that generated the CSV data sets. It represents a category that was removed from the survey data to reduce identifiability."
  },
  {
    "objectID": "doc_missing_and_ordinal.html#conclusion",
    "href": "doc_missing_and_ordinal.html#conclusion",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThe DRS survey data was rendered as a CSV file with a separate metadata xlsx file to convey the complex structure of the data, including the order of factors and the various types of missing data.\nAs a parting note, please utilize the core function, drs_read() as it produces a properly structured data set. This function has an argument that allows you to either see all the various types of missing data (ie. anything encoded with angle brackets &lt;...&gt;), or to convert these all to the special R value NA. This latter conversion is desirable when running an analysis or rendering plots where the types of missing data are not of concern. In such cases, analytical and plotting functions like survey::svydesign() or ggplot2::ggplot() can uniformly interpret these values as “non-responses”."
  },
  {
    "objectID": "doc_missing_and_ordinal.html#footnotes",
    "href": "doc_missing_and_ordinal.html#footnotes",
    "title": "Types of Missing Data and Ordinal Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe hash value stored in this txt file was produced by running a SHA-256 bit cryptographic algorithm, using digest::digest(), on the CSV file when it was initially created. In simpler terms, this hash value provides a way to confirm that the original data file has not been altered in transit from the computer that generated the csv file (ie. the author’s computer) to the end user’s computer. Due to the nature of electronic storage, data can become corrupted, either in the process of transmitting the data over the internet, reading or writing it, or as the hardware itself is exposed to cosmic radiation that alters the circuitry. By applying a cryptographic or hash function on a file, we are able to provide a brief description of the content in a succinct form: a 64-character series of numbers and letters. Should an error in the data occur, the cryptographic hash function applied to this error-ridden data set would most likely yield a different hash value. While there is a possibility that two files will share the same hash value, this is highly, highly improbable. As such, this is a simple and quick way to confirm that you have a good copy of the DRS data set.↩︎"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Delta Residents Survey",
    "section": "",
    "text": "https://quarto.org/docs/publishing/github-pages.html#publish-command"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "The hyperlinks presented in the list below each link to a separate chapter with detailed documentation directed at researchers using the DRS data set.1\n\n\n\n\nThe {cdrs} Package: An R package to facilitate reproducible analysis of the DRS data set.\nOrdinal & Missing Data: This chapter describes how the DRS data set has special metadata that describes the underlying data structure. While this chapter only briefly discusses the “code” per se, it more fully describes how to interpret the data we provide.\nCore Functions: The core functions to load the DRS 2023 data set. Notice: This document contains important pseudo-code to aid non-R users in reproducing the core reading function. However, the actual code here has been superseded by the new {cdrs} package listed above."
  },
  {
    "objectID": "documentation.html#source-code",
    "href": "documentation.html#source-code",
    "title": "Documentation",
    "section": "",
    "text": "The hyperlinks presented in the list below each link to a separate chapter with detailed documentation directed at researchers using the DRS data set.1\n\n\n\n\nThe {cdrs} Package: An R package to facilitate reproducible analysis of the DRS data set.\nOrdinal & Missing Data: This chapter describes how the DRS data set has special metadata that describes the underlying data structure. While this chapter only briefly discusses the “code” per se, it more fully describes how to interpret the data we provide.\nCore Functions: The core functions to load the DRS 2023 data set. Notice: This document contains important pseudo-code to aid non-R users in reproducing the core reading function. However, the actual code here has been superseded by the new {cdrs} package listed above."
  },
  {
    "objectID": "documentation.html#survey-methods",
    "href": "documentation.html#survey-methods",
    "title": "Documentation",
    "section": "Survey Methods",
    "text": "Survey Methods\nIn addition to the documentation below, we also provide a summary of our methods in the Summary Report, available on the home page.\n\nSurvey Weights and Complex Survey Design: A brief introduction to survey weights and, more broadly, survey design. We will attempt to answer: what are weights, why are they important, and how should they be used in the DRS?"
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are separate code repositories with their own sets of documentation as well. The documentation therein are directed at those interested in augmenting the source code, rather than simply utilizing it. If you wish to address bugs or make contributions, we refer you to the GitHub repositories. There are currently two public-facing repositories: the one from which this website was created (which also contains ‘core functions’ for posterity), and the package repository. The latter repository is where the bulk of the tools for the analysis of the DRS data are stored.↩︎"
  },
  {
    "objectID": "doc_weights.html",
    "href": "doc_weights.html",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "",
    "text": "In this chapter we will explore the importance and role of survey weights in the Delta Residents Survey (DRS), as well as a broader discussion on complex survey design. In general, quantitative inferential survey analysis involves a special branch of statistics that accounts for the unique qualities and assumptions of survey methodologies. Be advised that a proper statistical analysis cannot be conducted without the consideration of our complex survey design.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this document is in draft form! Some of the technical notes and citations have yet to be reviewed. Although this chapter was prepared by someone who is trained in statistics, the author is not a subject matter expert in the field of statistics. The guidance and information provided below should be considered provisional."
  },
  {
    "objectID": "doc_weights.html#introduction",
    "href": "doc_weights.html#introduction",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "",
    "text": "In this chapter we will explore the importance and role of survey weights in the Delta Residents Survey (DRS), as well as a broader discussion on complex survey design. In general, quantitative inferential survey analysis involves a special branch of statistics that accounts for the unique qualities and assumptions of survey methodologies. Be advised that a proper statistical analysis cannot be conducted without the consideration of our complex survey design.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this document is in draft form! Some of the technical notes and citations have yet to be reviewed. Although this chapter was prepared by someone who is trained in statistics, the author is not a subject matter expert in the field of statistics. The guidance and information provided below should be considered provisional."
  },
  {
    "objectID": "doc_weights.html#drs-complex-survey-design",
    "href": "doc_weights.html#drs-complex-survey-design",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "DRS Complex Survey Design",
    "text": "DRS Complex Survey Design\nThe Delta Residents Survey involves a probability-based complex sampling plan with stratified sampling across geographical zones and systematic application of survey weights to adjust for variable inclusion probabilities and response rates. This plan is considered a probability sampling method because it involves random selection processes within predefined strata (ie. the Delta zones) and applies systematic weighting. This method ensures that every household has a known probability of selection, allowing for the representation of the entire population within the survey sample."
  },
  {
    "objectID": "doc_weights.html#why-are-weights-important-and-what-are-they",
    "href": "doc_weights.html#why-are-weights-important-and-what-are-they",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "Why are weights important, and what are they?",
    "text": "Why are weights important, and what are they?\nIn order to mitigate issues with bias, we use post-hoc survey weights to estimate the survey population (Heeringa, West, and Berglund 2017, p38). Weights are often used to adjust for differences in selection probabilities, non-response, self-selection, and to align the sample more closely with the population structure. Weights are needed in most surveys because survey sampling plans usually deviate from Simple Random Sampling (SRS) (in which every unit in the population has an equal chance of selection, often impractical or insufficient for complex survey objectives).\n\nRelative to SRS, the need to apply weights to complex sample survey data changes the approach to estimation of population statistics or model parameters. Also relative to SRS designs, stratification, cluster sampling, and weighting all influence the sizes of standard errors for survey estimates. (Heeringa, West, and Berglund 2017, p26)\n\nIn other words, various approaches to the design of the survey sampling procedure introduce effects on the accuracy and precision of survey estimators (ibid, 26). In the DRS, survey weights are employed to account for different inclusion probabilities due to stratification (ie. the differential sampling between Delta zones), and due to external factors like non-response or self-selection bias.1\nIn the excerpt below, Heeringa et al discuss one approach to conceptualizing how weights relate to individual survey responses:\n\nA simple but useful device for “visualizing” the role of case-specific weights in survey data analysis is to consider the weight as the number (or share) of the population elements that is represented by the sample observation. Observation i, sampled with probability \\(f_i = \\frac{1}{10}\\), represents 10 individuals in the population (herself and 9 others). (Heeringa, West, and Berglund 2017, p38)\n\nIn summary, survey weights can be conceptualized as a numeric score that increases or decreases each respondent’s importance in building population estimators, like the mean length of residency or the proportion of a categorical variable like types of housing. Survey weights are an important (partial) corrective to biases introduced by a complex survey design and external factors. Given our intent to correctly represent the views expressed by residents of the Delta, we should use survey weights."
  },
  {
    "objectID": "doc_weights.html#the-construction-of-drs-weights",
    "href": "doc_weights.html#the-construction-of-drs-weights",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "The Construction of DRS Weights",
    "text": "The Construction of DRS Weights\nThe DRS survey weights were constructed by Marketing Systems Group (MSG).\n\nWeights for this survey were computed using the WgtAdjust procedure of SUDAAN, which relies on a constrained logistic model to predict the likelihood of response as a function of a set of explanatory variables (Marketing Systems Group and Fahimi 2023).2\n\nA more expansive description of the weighting methodology was prepared by MSG and is supplied in the 2023 Summary Report available on the home page."
  },
  {
    "objectID": "doc_weights.html#using-drs-weights-an-example",
    "href": "doc_weights.html#using-drs-weights-an-example",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "Using DRS Weights: An Example",
    "text": "Using DRS Weights: An Example\nIn the following code chunks, we demonstrate the basic process of developing weighted proportions or frequencies in the R-language for a single variable from the survey, Q2. This demonstration is provided to explain how to approach such an analysis, however we encourage you to use the {cdrs} package in your actual analysis (see Section 6).\nIn the code that follows, we make the following assumptions:\n\nThe R object data represents the full survey results from the public version of the DRS data set.\nThe column Q2 which represents question 2 on the survey, has missing values represented as NA. In other words, the different types of missing values have already been reduced to a uniform NA value. We also assume Q2 started out as the R class factor. Remember, R factors are a data type that represents categorical information with a limited number of levels, eg. “Lives in the Delta”.\nWe primarily rely on two packages: survey, and tidyverse. We assume you have a basic understanding of the {tidyverse}, such as the role of pipes %&gt;%.\n\nFirst, we create a subset of the data in which we’re interested. We will isolate three columns: the variable of interest Q2, the survey strata Zone, and the final weights WTFINAL.\n\nsubsetQ2 &lt;- data %&gt;%\n  # Select the variable(s) of interest, and the Zone and weights columns\n  select(Q2, Zone, WTFINAL) %&gt;%\n  # Remove NA values in the weight column\n  filter(!is.na(WTFINAL)) %&gt;%\n  # Remove missing values in the variable column(s)\n  filter(!is.na(Q2)) %&gt;%\n  # Drop factors that are unused.\n  mutate(Q2 = fct_drop(as_factor(Q2)))\n\nSecond, we stipulate the “survey design” which describes the design of our complex survey in a format that statistical functions from the {survey} package can utilize. Here, we will explain our reasoning for how we defined each parameter in survey::svydesign(). For more details, please see the documentation for this function.\n\nids: This refers to cluster sampling, a method we did not employ. The ~1 simply indicates there is no clustering.\nfpc: This defines the ‘finite population correction’. According to Heeringa et al. (2017), the fpc “reflects the expected reduction in the sampling variance of a survey statistic due to sampling [without replacement]” (p24). We would assume our fpc has little effect (and approaches 1) when our sample is much smaller than the total population (N) we’re estimating. For our survey’s three zones, only Zone 1 has a sample that might be considered important for the fpc: we have 326 responses out of a total population of 11,727 (2.78%). We are thus presented with two options: specify the fpc as NULL and that no correction needs to be made (given that 2.78% may still be considered sufficiently small), or specify the fpc to account for the different populations by Zone.3 In practice, we did not see a difference when estimating weighted proportions/frequencies.\ndata: The subsetted data. Importantly, this data cannot have any missing values for the weight column, WTFINAL.\nstrata: The column that specifies the strata, in our case Zone.\nweights: The column that specifies the weights, in our case WTFINAL.\n\n\ndesignQ2 &lt;- svydesign(\n  # This arg specifies cluster ids. In our case, the DRS has none.\n  ids = ~1,\n  # The inclusion of the finite population correction, in this example, was not demonstrated affect the frequencies, so they are excluded here.\n  fpc = NULL,\n  # The cleaned data set, including the removal of missing values.\n  data = subsetQ2,\n  # Specify the strata (in our case Zone geographies)\n  strata = ~ Zone,\n  # Specify the column with weights.\n  weights = ~ WTFINAL\n)\n\nFinally, we can calculate the weighted proportions of the various categories in Q2.\n\nprop &lt;- survey::svymean(\n  # x is the formula\n  x = ~ Q2,\n  design = designQ2)\n\n\n\n\n\n\n\nFormulas\n\n\n\nFor more advanced users, be aware that you can also write the formula in a way that’s more programmatic. For instance, if you stored the name of your variable of interest, Q2, in a vector (eg. var_ &lt;- \"Q2\"), you could write the formula for svymean (or any other formula-taking function) with as.formula(). In this case, we would specify it like this:\n\nsvymean(x = as.formula(paste0(\"~\", var_)), \n        design = designQ2)\n\n\n\nHere is another example, this time calculating weighted frequencies with a breakdown of the variable Q2 by Zone. To accomplish this, we can use svytable().\n\nfreq &lt;- survey::svytable(\n  # Note, `x` is now `formula`,\n  # and for fun, we specified the formula using `as.formula`.\n  # Alternatively, we could have supplied `~ Q2 + Zone`\n  formula = as.formula(paste0(\"~\", \"Q2\", \" + \", \"Zone\")), \n  design = designQ2)"
  },
  {
    "objectID": "doc_weights.html#sec-package",
    "href": "doc_weights.html#sec-package",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "Using the cdrs package",
    "text": "Using the cdrs package\nIn practice, we highly recommend you use the package {cdrs} to reduce chances of user error, to reduce coding, and to create more reproducible code documents. Learn more:\n\nDocumentation\nGitHub Repository"
  },
  {
    "objectID": "doc_weights.html#footnotes",
    "href": "doc_weights.html#footnotes",
    "title": "Survey Weights & Complex Survey Designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNon-response introduces bias when households selected to be surveyed do not respond. This may occur regardless of the household’s willingness to participate, eg. socio-economic barriers effect the timely delivery of surveys. Whereas, self-selection bias occurs when the likelihood of participation in the survey is not random but influenced by characteristics of the individuals. This bias can lead to certain viewpoints or demographic profiles being overrepresented or underrepresented in the survey data.↩︎\nMSG cites the SUDAAN Manual (Research Triangle Institute 2012), and they likely meant to refer to the WTADJUST procedure. A copy of the SUDAAN manual can be made available by the DRS team upon request.↩︎\nIn order to specify the fpc, this is the approach to take: 1) Create a data.frame with the total population (N = c(11727, 540340, 166085)) for each Zone (Zone = factor(c(1, 2, 3))). 2) Merge the DRS data with this data.frame using dplyr::left_join. 3) Specify svydesign such that the parameter fpc equal ~ N, where N is the column with the total population for the three Zones.↩︎"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Survey Overview",
    "section": "",
    "text": "The following information was originally published in the 2023 Summary Report (available on the home page)."
  },
  {
    "objectID": "overview.html#introduction",
    "href": "overview.html#introduction",
    "title": "Survey Overview",
    "section": "Introduction",
    "text": "Introduction\nCalifornia relies on the Sacramento-San Joaquin Delta, the largest estuary on the west coast, that serves as a hub for freshwater resources distributed across the state, a biodiverse ecosystem, productive agricultural land, and a crossroads for statewide infrastructure and transportation networks. Historic towns, cultural resources, and recreational opportunities pepper the rural interior Delta, which lies only miles away from the significant metropolitan areas that make up the urban Delta perimeter. In 2009, the Delta Reform Act created the Delta Stewardship Council to advance California’s “coequal goals” for the Delta: “a more reliable statewide water supply and a resilient Delta ecosystem – in a manner that protects and enhances the unique characteristics of the Delta as an evolving place where people live, work, and recreate” (Wat. Code, § 85000 et seq.). The Council houses the Delta Science Program, which is tasked with providing the best possible scientific information to inform water and environmental management decisions for the Delta that aim to advance the coequal goals.\nDespite statutory guidance calling for a complex balance of competing needs in the estuary and science-informed decision-making, the social and human dimensions of the Delta have been vastly understudied to date, in comparison to the physical and ecological components of the system (Bidenweg; Delta Independent Science Board, Monitoring Enterprise Review; Delta Independent Science Board, Review of Research on the Sacraemnto-San Joaquin Delta as an Evolving Place). While decades of monitoring the ecological health of the system have informed management approaches for ecosystem recovery, there has been significantly less attention to monitoring or evaluating the social health of the estuary, including how people influence ecological outcomes of interest.\nIn contexts like the Delta where people deeply impact and are impacted by the state of the natural system, understanding the people who live, work, play and depend on the environment is essential to developing effective and equitable management approaches. Moreover, people are at the heart of designing, supporting and implementing estuary recovery and resilience- building efforts, which are necessary in order to meet the state’s coequal goals for the Delta. Understanding and tracking change in the human dimensions of the estuary– such as residents’ opinions on regional priorities and concerns, stewardship behaviors, and experiences– will be essential to achieving the coequal goals.\nThe development of the 2023 Delta Residents Survey is one of multiple recent efforts supported by the Delta Stewardship Council’s Social Science Integration Team and the Bay-Delta Social Science Community of Practice to begin better understanding and incorporating the human dimensions of the Delta into decision making. The Delta Residents Survey (DRS) was designed by a team of social science researchers working closely with Delta Stewardship Council staff, other partner state and local agencies, and community partners.\nThe DRS had four substantive research aims:\n\nCharacterize residents’ sense of place;\nAssess well-being of a diverse and evolving population living in the region;\nUnderstand residents’ experiences and perceptions of environmental and climate changes across the estuary;\nEvaluate residents’ civic engagement and perceptions of governance in the region.\n\nThe data were collected via a survey (available online and print version), with survey invitations sent by mail to a random sample of 82,000 households in the rural “Primary Zone” of the Delta (survey Zone 1), the suburban and urban “Secondary Zone” of the Delta (survey Zone 2) and Delta-adjacent “EJ Communities” in South Sacramento and South Stockton (survey Zone 3). The survey was available in English and Spanish. The survey included 43 multiple choice and short response questions, based on well-tested survey questions with input from the survey advisory group and Delta community members to ensure questions were appropriately locally tailored. See Table 1 below for a summary of the survey sections and Appendix A for the full survey tool.\nA total of 2,208 usable responses were received, constituting a 2.9% response rate, which is better than recent average response rates for randomized household surveys (CSU Institute for Social Research), and a margin of error of plus or minus 2.1%, given a 95% confidence interval. Survey analyses are based on weighted data to ensure results reflect demographically-representative sentiments. All details on methodology for survey design, distribution, data weighting and analysis can be found in Appendix B [of the Summary Report]."
  },
  {
    "objectID": "doc_drs_functions.html",
    "href": "doc_drs_functions.html",
    "title": "DRS Functions",
    "section": "",
    "text": "In this document, we review the functions in the file “drs_functions.R”. This script contains the primary functions to help you begin to navigate the 2023 survey data. At a later date, we intend to convert the functions in this script into a package. As such, you may find some additional documentation in the script itself using R oxygen2 style comments, prefacing each function declaration."
  },
  {
    "objectID": "doc_drs_functions.html#sec-introduction",
    "href": "doc_drs_functions.html#sec-introduction",
    "title": "DRS Functions",
    "section": "",
    "text": "In this document, we review the functions in the file “drs_functions.R”. This script contains the primary functions to help you begin to navigate the 2023 survey data. At a later date, we intend to convert the functions in this script into a package. As such, you may find some additional documentation in the script itself using R oxygen2 style comments, prefacing each function declaration."
  },
  {
    "objectID": "doc_drs_functions.html#quickstart",
    "href": "doc_drs_functions.html#quickstart",
    "title": "DRS Functions",
    "section": "2 Quickstart",
    "text": "2 Quickstart\nA close study of this document is not necessary to load the DRS data set. Simply follow the commands below to start using the data.\n\nDetermine your local file path to the directory containing the data.\nOpen your script, write and then execute, source(\"drs_functions.R\").\nWith your file path, run data &lt;- drs_read(\"YOUR/PATH/HERE\"). Alternatively, if you just want to start plotting the data, run data &lt;- drs_read(\"YOUR/PATH/HERE\", convert_to_NA = T)\n\n\n\n\n\n\n\nTip\n\n\n\nThe “ReadMe” file in the repository has a more detailed Quickstart guide."
  },
  {
    "objectID": "doc_drs_functions.html#notes-on-coding-style",
    "href": "doc_drs_functions.html#notes-on-coding-style",
    "title": "DRS Functions",
    "section": "3 Notes on Coding Style",
    "text": "3 Notes on Coding Style\nThis document assumes you have familiarity with the {tidyverse} family of packages. Specifically, it assumes that you understand:\n\nPipes %&gt;%.\nThe *apply family of functions and how they are mimicked in {purrr} with functions like map().\nTibbles.\n\nIt also assumes you have at least a conceptual understanding of “regular expressions.”\n\n\n\n\n\n\nRegular Expressions\n\n\n\nAs a short summary, regular expressions (aka regex) are used to find patterns in strings (ie. text). Using special symbols placed in a certain sequence, they can help to identify patterns in text. For instance, to identify the text \"&lt;Missing&gt;\", we might specify a search for a pattern of angle brackets. We could use this regex, \"^\\\\&lt;.+\\\\&gt;$\" which essentially identifies if a piece of text has a pattern that matches the following description: the first character is an left angle bracket; followed by any combination of characters; and concludes with a right angle bracket. This example is not meant to teach you regex, but rather demonstrate purpose that regex fills: a means to detect textual patterns in character vectors."
  },
  {
    "objectID": "doc_drs_functions.html#sec-outline",
    "href": "doc_drs_functions.html#sec-outline",
    "title": "DRS Functions",
    "section": "4 Outline of Functions",
    "text": "4 Outline of Functions\n\ndrs_as_NA()\nThis function works in complement with drs_read (although it could be of use by itself). Its purpose is to convert variable values/levels that encode a special type of “missingness”, including survey responses like “Decline to answer” or “Not Applicable” into an R-friendly format. This function can identify special cases of “missingness” because the DRS data set stores these values within . This allows the DRS data set to provide nuanced information about the manner in which responses were recorded, while also allowing R to more easily execute analyses that depend on a standardized way to convey missing data in the form of NA.\nTechnically, this function takes a data.frame or tibble as an input and then applies an algorithm on each column. However, the algorithm is only applied to “character” or “factor” columns. It searches each of these columns for values that match a regex pattern consisting of text enclosed between angle brackets, eg. \"&lt;Decline to answer&gt;\". It then converts these values to standard NA values. In the case of factors, this algorithm also drops missing levels (eg. the level \"&lt;Decline to answer&gt;\" is no longer recorded when running levels()).\ndrs_read()\nThis is the primary function to load the DRS data set. It operates by combining the metadata stored in the data dictionary with the data itself to yield a more complex data structure. The output is a standard tibble. It depends on drs_as_NA.\nThis function applies a number of tests to ensure that the data conform to the quality and structure of the original data as specified by the DRS team. Because we decided to limit public access to personally identifiable information, and because the data were originally stored in a format that requires intimate knowledge of both the SPSS sav data format and the R {haven} package, the DRS team determined that a modified data set would be more appropriate for public use. As there is no standard way to convey the original data structure in R without additional knowledge, this function provides a simple approach to either reproducing or disregarding the complexity of the original data set. As such, we highly encourage DRS data users to utilize the algorithm specified in this function."
  },
  {
    "objectID": "doc_drs_functions.html#sec-pseudocode",
    "href": "doc_drs_functions.html#sec-pseudocode",
    "title": "DRS Functions",
    "section": "5 Function Pseudocode",
    "text": "5 Function Pseudocode\nThis section provides an overview of the algorithm present in the DRS functions script. These algorithms are written in pseudocode to provide non-R users with a method to translate the R script into the language of your choice. Given that languages like Python remain a popular data science language, and given our lack of expertise outside of the R language, we hope this prosaic description offers some utility.\n\n5.1 drs_read\n\nMake sure the argument .path satisfies some of the requirements of a file path. Given differences between Windows and Unix based filesystems, it makes no assumptions about the directory structure. So, it merely makes sure the path is a character (as even the output of file.path is a character), and that the vector supplied is of length 1.\nLoad packages. Using a combination of lapply and invisible we can silently load packages that have not been attached to the R environment. All but one of the packages are standard packages within the tidyverse. This lone package is {digest}, which serves as a means to validate the data checksum (described below).\nValidate files. This is the first formal section of this function (ie. listed in the document outline). It derives a list of files in the directory path specified by .path. It creates a tibble listing all required files. Then using map_vec to search each file name in the directory for the regex of one of the required files. This allows us to match file names to the required files needed to load DRS data. In other words, here we identify that there is a file for the data dictionary, the data itself, and the hash code (discussed later). This section concludes with an if statement that verifies that all these files are present.\nValidate data. Read the checksum hash code. Make certain that the hash code follows the structure of a SHA-256 hash with 64 characters. Apply the SHA-256 cryptographic hash function on the actual DRS data stored in the directory as a csv file. The concluding if statement simply compares the checksum hash code to the newly created hash for the csv data. In summary, this section affirms that the data hasn’t become corrupted somehow.\nLoad dictionary. Read the data dictionary from the xlsx file. Then, clean up the dictionary so it doesn’t have empty rows, and each item is affixed to one of the DRS column variables. Notice the naming structure of the data dictionary table headers. Among these headers includes “name” and “value”. This naming scheme is meant to mimic the dictionaries generated by SPSS. These key-pairs will eventually help us structure the DRS data (ie. the output).\nLoad data. Our primary objective is to correctly assign columns to their appropriate data types (eg. character, factor, numeric, or date-time) as we read the DRS data into the R environment. Thus, before using read_csv, we have to do some administrative work to accomplish this objective. We want to first identify the order that columns/variables in the csv appear, and then their order in the data dictionary. We want to ensure that we can correctly match each column of the csv to their appropriate data type in the dictionary. This produce the object col_order. Then we process the data dictionary to get a tibble of each variable and their data type, producing r_class. Note that read_csv accepts an argument col_types that specifies the R class of each column. The argument accepts a string, with each character in the string corresponding to the sequence of columns. So, fff would suggest we are reading a table with three factor columns, or Tc would suggest we are reading a table with two columns: first a date-time then a character column. This section concludes with read_csv being invoked with the col_types specified according to the data dictionary.\nSet the order of ordinal values. By inspecting the data dictionary carefully, you should notice that some factor variables have a specified encoding. This column identifies the numeric encoding that the original data set we received included. As the original data set was generated in SPSS and read into R using {haven}, we had the option of viewing the data as either encoded numbers or textual values. For instance, question Q1_0 has three possible factors: Decline to answer, No, and Yes. These values could alternatively represented by numbers: 99, 0, and 1 respectively. While these encodings do not always correspond to a specific order or sequence, some variables do have some where the order matters. For example, question Q10 has a diverging scale of possible responses, that range from Very satisfied to Very dissatisfied (along with an option to decline to answer). In this case, and in cases with sequence like income groupings that scale from low income to high income, the order matters. Thus, the objective of this section is to preserve the order.\nWe begin by splitting the data dictionary by Variable. This yields a list object, vars_. Then, using map we derive a list with elements either consisting of NULL (ie. variables without a specific order), or a tibble with the appropriate encoding. After cleaning up the output (fcts_), we use map again to produce a complete data set with correctly ordered ordinal variables. This map function relies on fct_relevel to sort the variable’s factors according to the order presented in the data dictionary (and stored in the variable fcts_).\nConvert NAs. This section is conditional, meaning it only runs if the user specifies the argument convert_to_NA as TRUE in the function call. Using one of the other DRS functions, drs_as_NA, we convert variables with values that match the regex pattern \"^\\\\&lt;.+\\\\&gt;$\" (eg. &lt;Decline to answer&gt;).\nAfter these bracketed missingness values are converted to NA values, we then convert variables described as “factor - numeric” in the dictionary to simple numeric variables. This special case only appears twice in the whole survey, and only once in the publicly available data set. One of these, Q1a, asks respondents about how long they have lived in the Delta. Responses are typed entries that indicate a time span in terms of years. In other words, respondents type in a number. However, respondents may have also skipped the question (ie. “&lt;Declined to answer&gt;”) or stated that this question doesn’t apply to them (perhaps because they do not identify as living in the Delta; their response appears as “&lt;Not applicable&gt;”). These two non-numeric responses are identified as “factors” initially. But once they are converted to NA values, we can treat the variable as a numeric data type.\nReorganize the data and return it. As the final step of this function, the data set has its columns re-ordered to match the order of variables in the data dictionary. The final output of the function is a tibble."
  },
  {
    "objectID": "doc_cdrs_package.html",
    "href": "doc_cdrs_package.html",
    "title": "The {cdrs} Package",
    "section": "",
    "text": "The {cdrs} (pronounced “cedars”) package for R is intended to make your analysis of the DRS data easier and more reproducible. Below, we discuss how to install it, and describe some of the key functions of this package. To examine the source code, go to the github repository here."
  },
  {
    "objectID": "doc_cdrs_package.html#introduction",
    "href": "doc_cdrs_package.html#introduction",
    "title": "The {cdrs} Package",
    "section": "",
    "text": "The {cdrs} (pronounced “cedars”) package for R is intended to make your analysis of the DRS data easier and more reproducible. Below, we discuss how to install it, and describe some of the key functions of this package. To examine the source code, go to the github repository here."
  },
  {
    "objectID": "doc_cdrs_package.html#installing-and-updating",
    "href": "doc_cdrs_package.html#installing-and-updating",
    "title": "The {cdrs} Package",
    "section": "Installing and Updating",
    "text": "Installing and Updating\nThis package is not yet available on the official package repository of CRAN. As such, you will need to install it manually using a function from the package {devtools}. The {devtools} package helps R programmers do several tasks such as write R packages, but in this case, it is used to install the {cdrs} package from GitHub.\n\n1. Set up devtools\nFirst, make sure you have {devtools} installed. You can run this script to see if you have it installed on your machine, if it not it will download and install it from CRAN. If you know you don’t have it, just run install.packages(\"devtools\").\n\nif (!requireNamespace(\"devtools\", quietly = TRUE)) {\n    install.packages(\"devtools\")\n    message(\"devtools has been successfully installed.\")\n} else {\n    message(\"devtools is already installed.\")\n}\n\n\n\n2. Install cdrs\nSecond, install {cdrs} from GitHub.\n\ndevtools::install_github(\"ktomari/cdrs\")\n\n# you can then load it like any other package.\nlibrary(cdrs)\n\n\n\n3. Update cdrs\nAs of this writing, the package {cdrs} is actively being updated. Since it is not currently available on CRAN, you will have to check for updates manually. You may do so by checking our News page to see if there have been updates. When updates are available, simply repeat step 2) devtools::install_github(\"ktomari/cdrs\"). This will download it again from GitHub."
  },
  {
    "objectID": "doc_cdrs_package.html#key-features",
    "href": "doc_cdrs_package.html#key-features",
    "title": "The {cdrs} Package",
    "section": "Key Features",
    "text": "Key Features\n\nReading DRS Data\nYou can read the DRS data in with cdrs_read() which has two arguments: the path to the zip file or directory which contains the DRS data, and whether or not to convert &lt;Missing&gt; values into NA.\n\ndrs_data &lt;- cdrs::cdrs_read(\"Your/Path/Here\", convert_to_NA = T)\n\nIf you want to see an example of this in action without downloading the public data set, use cdrs_read_example()\n\n\nDRS with {survey}\nWe rely on the {survey} package for much of the backend calculations of the DRS survey. This package is critical because it allows us to make proper estimations given the unique characteristics of survey statistics. You can learn more about how we approach survey statistics in our documentation.\nThe first step requires us to create a data subset, followed by a “survey design object” which specifies the complex survey design that the DRS uses. We provide the conceptual details of how we specify this object in the Weights and Complex Survey Design page, whereas below, we simply demonstrate its implementation with {cdrs}.\n\n# First we create a data subset of the variables we're interest in.\n# The cdrs_subset() function subsets the variables of interest,\n# as well as the Zone and WTFINAL columns which we'll need later.\n# Note, you may pass column names either as a vector of quoted column names, or\n# as unquoted column names, as show below.\ndrs_subset &lt;- cdrs::cdrs_subset(drs_data, Q2)\n\n# Second, we create the survey design object, which can later be used with\n# {survey} functions like svytable.\nsvydes_obj &lt;- cdrs::cdrs_design(drs_subset, set_fpc = T)\n\nGiven this, we can reproduce the first example in the Weights and Complex Survey Design page. This calculated the weighted proportions of Q2.\n\nprop &lt;- survey::svymean(\n  # x is the formula\n  x = ~ Q2,\n  design = svydes_obj)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "The Delta Residents Survey (DRS) was a community survey conducted in the first quarter of 2023, reaching over 2,300 Delta and Delta-adjacent residents of the California Delta. The DRS had four substantive research aims:\nAn initial analysis of the survey results are provided in the Summary Report (see below)."
  },
  {
    "objectID": "index.html#sec-summaryreport",
    "href": "index.html#sec-summaryreport",
    "title": "Introduction",
    "section": "Summary Report",
    "text": "Summary Report\nThe summary report is temporarily available here, and will become available on the Delta Stewardship Council website in the future. A screen-reader friendly version will be uploaded as well. Additional materials will be posted as they become available. Please check back in early 2024.\nYou can also view the video, listen to the audio, or read the transcript of our report to the Delta Stewardship Council meeting on October 26, 2023. Look for Agenda item #10."
  },
  {
    "objectID": "index.html#package-repository",
    "href": "index.html#package-repository",
    "title": "Introduction",
    "section": "Package Repository",
    "text": "Package Repository\n\n\n\nIn order to facilitate other researchers to better utilize the DRS data, we are creating an R-language package, {cdrs}. While its still a work in progress, the core functions are already available. We hope with this package that data users will be able to create reproducible code. Visit the package repository and the documentation to learn more."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\nThe public data set is now available on openICPSR, a database of social, behavioral, and health sciences research data. The Digital Object Identifier (DOI), which serves as the URL for the data, is https://doi.org/10.3886/E195447V1.\nTo protect respondent privacy, certain geographic, demographic, and survey response variables belonging to the original dataset are restricted from general public dissemination. Users interested in obtaining these data must complete a Data Sharing Agreement for the use of confidential data, submit a Data Request Form to the research team that specifies the reasons for their request, research questions, and specific variables they seek to use, and obtain IRB approval or notice of exemption for their research. To learn more, first review the Data Sharing Agreement available in project documents and then review available materials in the “Restricted Use Data Information” folder.\n\nQuickstart\nWhile there are several documents available on our project openICPSR page, the data itself are stored in the “DRS public data… .zip” compressed folder.\nTo load the data, follow the steps outlined in the {cdrs} documentation."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Introduction",
    "section": "Contact",
    "text": "Contact\nIf you have questions, please contact any of the following project members. Please note that we have obfuscated the email address to reduce spam. A listing of domain names (ie. @example.com) is shown in a table that follows.\n\nAnnie Merritt, Delta Stewardship Council. Please contact them with the name annie.merritt at their council email address.\nDr. Jessica Rudnick. Please use the name jessica.rudnick15 at their non-institutional domain.\nKenji Tomari, UC Davis. Please contact them with the name ktomari at their academic institutional domain.\n\n\n\n\nInstitution\nDomains\n\n\n\n\nUC Davis\nucdavis.edu\n\n\nDelta Stewardship Council\ndeltacouncil.ca.gov\n\n\nUnlisted\ngmail.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The following information was originally published in the 2023 Summary Report (available on the home page)."
  },
  {
    "objectID": "about.html#research-team",
    "href": "about.html#research-team",
    "title": "About",
    "section": "Research Team",
    "text": "Research Team\n\nDr. Jessica Rudnick - Social Science Extension Specialist, CA Sea Grant & Delta Stewardship Council\nKenji Tomari, MS. MA. - PhD Student, University of California Davis\nDr. Kristin Dobbin - Assistant Professor of Cooperative Extension, University of California Berkeley\nDr. Mark Lubell - Professor, University of California Davis\nDr. Kelly Biedenweg - Professor, Oregon State University"
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank the many community groups and Delta residents who made this research possible by participating and providing genuine input in the community meetings, focus groups, interviews, and surveys that informed this work and the results presented in this report. In addition to Delta residents, many additional project partners contributed to the success of this effort. The research team would like to thank the following people and groups for their support:\n\nDelta Stewardship Council and Delta Science Program\nThe Delta Stewardship Council’s Delta Science Program was established to provide scientific information and syntheses to inform environmental and water decision-making in the Delta. The program fulfills its mission by funding research, synthesizing and communicating scientific information to policymakers and decision-makers, promoting independent scientific peer review, and coordinating with Delta agencies to promote science-based adaptive management. The Delta Science Program provided funding for the Delta Residents Survey research project to fill a research gap identified by the 2020 Social Science Strategy for the Sacramento-San Joaquin Delta (Bidenweg) and the 2022-2026 Science Action Agenda (DeltaScienceProgram).\nSpecial acknowledgements of the following Delta Stewardship Council staff who supported the DRS effort to exceptional degrees are merited. Thank you to Lita Brydie (former Program Analyst) for her work through her Master’s Capstone project to support qualitative data analysis informing this project, Annie Merrit (Environmental Scientist) for her help in the community outreach campaign, Rachael Klopfenstein (Program Manager) for her management of the funding contract for this project, Louise Conrad (former Deputy Executive Officer of Delta Science Program), Laurel Larsen (Delta Lead Scientist), and Jeff Henderson (Deputy Executive Officer for Planning and Performance Division) for their consistent support and attention to this research effort, and to Emily Ryznar, 2021 California Sea Grant State Policy Fellow within the Delta Science Program, for her contributions to background research efforts.\n\n\nSacramento State University Institute for Social Research\nThe Institute for Social Research (ISR) supports community partners in finding data-informed answers to their most pressing questions. ISR offers a broad range of expertise in conducting applied survey research and working with state government agencies to inform program and policies across the state. Special thanks to Julia Tomassili, Robert Rodriguez, Mayra Adame, Sebastian Cambre, and Shannon Williams on the ISR team who supported the Delta Residents Survey research effort by providing expertise on survey sampling and survey design, translation, programming, and distribution of the survey to respondents, and collating, cleaning and anonymizing survey data.\n\n\nSurvey Advisory Committee\nAdvisory committee members volunteered their time, expertise and effort to support the Delta Residents Survey research effort, meeting three times over the course of 2022 to advise on survey design, support community outreach and survey beta-testing, and provide feedback on draft survey materials and results. Committee members are not listed in any particular order.\n\nAjay Singh - Sacramento State University, Environmental Studies\nAlejo Kraus-Polk – UC Davis, Geography\nAlex Thompson - San Francisco Estuary Partnership (former 2021 California Sea Grant State Policy Fellow)\nCaleb Scoville – Tufts University Campbell Ingram – Delta Conservancy\nDarcie Luce – San Francisco Estuary Partnership\nDarcy Bostic – Rural Community Assistance Partnership\nErik Vink – Delta Protection Commission (former)\nGilbert Cosio – River Delta Consulting\nIsa Avanceña – Valley Vision\nJosue Medellín-Azuara - UC Merced, Agriculture Economics\nKrista Harrington – Oregon State University\nLita Brydie - former Delta Stewardship Council staff; Oregon State University Master’s Capstone project supported this research\nMegan Wheeler – San Francisco Estuary Institute\nMichael George – Delta Watermaster’s Office (former)\nRandy Fiorini - Former chair of the Delta Stewardship Council\nRosemary Kosaka – NOAA Fisheries Southwest Fisheries Science Center\nTim Stroshane – Restore the Delta\nVirginia Gardiner – Delta Protection Commission\nWilliam Muetzenberg – Public Health Advocates\nBrett Milligan- University of California Davis\nMatto Mildenberger- University of California Santa Barbara\nTanya Heikkila - University of Colorado, Denver\nDavid Trimbach- Oregon State University (former)\nKristi Matal - Office of the Delta Water Master\nBarbara Barrigan-Parilla – Restore the Delta\nBob Erlenbusch - Sacramento Regional Coalition to End Homelessness\nMatt Holmes - Little Manilla Rising\nSherri Norris - California Indian Environmental Alliance"
  },
  {
    "objectID": "about.html#funding",
    "href": "about.html#funding",
    "title": "About",
    "section": "Funding",
    "text": "Funding\nData collection for this project was funded by a contract agreement between the Delta Science Program, Delta Stewardship Council (Contract DSC-21143), California Sea Grant, and California State University Sacramento. The contents of this report may not necessarily reflect the official views or policies of the State of California."
  },
  {
    "objectID": "about.html#data-analysis",
    "href": "about.html#data-analysis",
    "title": "About",
    "section": "Data Analysis",
    "text": "Data Analysis\nWe are very grateful to Thomas Lumley for the {survey} package he developed in R to support complex survey analysis, and for the {Tidyverse} package developed by Pivot."
  }
]